# Big Data with PySpark

# Progress

## Introducation to PySpark

### Getting to know PySpark
[Link to Notebooks](https://github.com/ayushsubedi/big-data-with-pyspark/tree/main/notebooks/1_introduction_to_pyspark/1_getting_to_know_pyspark)

 - [x]	What is Spark, anyway?
 - [x]	Using Spark in Python
 - [x]	Examining The SparkContext
 - [x]	Using DataFrames
 - [x]	Creating a SparkSession
 - [x]	Viewing tables
 - [x]	Are you query-ious?
 - [x]	Pandafy a Spark DataFrame
 - [x]	Put some Spark in your data
 - [x]	Dropping the middle man
 
 ### Manipulating data 
 [Link to Notebooks](https://github.com/ayushsubedi/big-data-with-pyspark/tree/main/notebooks/1_introduction_to_pyspark/2_manipulating_data)
 - [x]	Creating columns
 - [x]	SQL in a nutshell
 - [x]	SQL in a nutshell (2)
 - [x]	Filtering Data
 - [x]	Selecting
 - [x]	Selecting II
 - [x]	Aggregating
 - [x]	Aggregating II
 - [x]	Grouping and Aggregating I
 - [x]	Grouping and Aggregating II
 - [x]	Joining
 - [x]	Joining II

### Getting started with machine learning pipelines
[Link to Notebooks](https://github.com/ayushsubedi/big-data-with-pyspark/tree/main/notebooks/1_introduction_to_pyspark/3_getting_started_with_machine_learning)
- [ ]	Machine Learning Pipelines
- [ ]	Join the DataFrames
- [ ]	Data types
- [ ]	String to integer
- [ ]	Create a new column
- [ ]	Making a Boolean
- [ ]	Strings and factors
- [ ]	Carrier
- [ ]	Destination
- [ ]	Assemble a vector
- [ ]	Create the pipeline
- [ ]	Test vs Train
- [ ]	Transform the data
- [ ]	Split the data

### Model tuning and selection
[Link to Notebooks](https://github.com/ayushsubedi/big-data-with-pyspark/tree/main/notebooks/1_introduction_to_pyspark/4_model_tuning_and_selection)
- [ ]	What is logistic regression?
- [ ]	Create the modeler
- [ ]	Cross validation
- [ ]	Create the evaluator
- [ ]	Make a grid
- [ ]	Make the validator
- [ ]	Fit the model(s)
- [ ]	Evaluating binary classifiers
- [ ]	Evaluate the model

# Cleaning Data with PySpark

### DataFrame details
- [ ] A review of DataFrame fundamentals and the importance of data cleaning.
- [ ] Intro to data cleaning with Apache Spark
- [ ] Data cleaning review
- [ ] Immutability and lazy processing
- [ ] Immutability review
- [ ] Using lazy processing
- [ ] Understanding Parquet
- [ ] Saving a DataFrame in Parquet format
- [ ] SQL and Parquet

### Manipulating DataFrames in the real world
- [ ] DataFrame column operations
- [ ] Filtering column content with Python
- [ ] Filtering Question #1
- [ ] Filtering Question #2
- [ ] Modifying DataFrame columns
- [ ] Conditional DataFrame column operations
- [ ] when() example
- [ ] When / Otherwise
- [ ] User defined functions
- [ ] Understanding user defined functions
- [ ] Using user defined functions in Spark
- [ ] Partitioning and lazy processing
- [ ] Adding an ID Field
- [ ] IDs with different partitions
- [ ] More ID tricks

### Improving Performance
- [ ] Caching
- [ ] Caching a DataFrame
- [ ] Removing a DataFrame from cache
- [ ] Improve import performance
- [ ] File size optimization
- [ ] File import performance
- [ ] Cluster configurations
- [ ] Reading Spark configurations
- [ ] Writing Spark configurations
- [ ] Performance improvements
- [ ] Normal joins
- [ ] Using broadcasting on Spark joins
- [ ] Comparing broadcast vs normal joins

### Complex processing and data pipelines

- [ ] Introduction to data pipelines
- [ ] Quick pipeline
- [ ] Pipeline data issue
- [ ] Data handling techniques
- [ ] Removing commented lines
- [ ] Removing invalid rows
- [ ] Splitting into columns
- [ ] Further parsing
- [ ] Data validation
- [ ] Validate rows via join
- [ ] Examining invalid rows
- [ ] Final analysis and delivery
- [ ] Dog parsing
- [ ] Per image count
- [ ] Percentage dog pixels











