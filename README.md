# Big Data with PySpark

# Progress

## Introduction to PySpark

[Certificate](https://www.datacamp.com/statement-of-accomplishment/course/4f60910c39332bd0cad02155c50d018b08456f9c)

### Getting to know PySpark

- [x]	What is Spark, anyway?
- [x]	Using Spark in Python
- [x]	Examining The SparkContext
- [x]	Using DataFrames
- [x]	Creating a SparkSession
- [x]	Viewing tables
- [x]	Are you query-ious?
- [x]	Pandafy a Spark DataFrame
- [x]	Put some Spark in your data
- [x]	Dropping the middle man
 
 ### Manipulating data 

- [x]	Creating columns
- [x]	SQL in a nutshell
- [x]	SQL in a nutshell (2)
- [x]	Filtering Data
- [x]	Selecting
- [x]	Selecting II
- [x]	Aggregating
- [x]	Aggregating II
- [x]	Grouping and Aggregating I
- [x]	Grouping and Aggregating II
- [x]	Joining
- [x]	Joining II

### Getting started with machine learning pipelines

- [x]	Machine Learning Pipelines
- [x]	Join the DataFrames
- [x]	Data types
- [x]	String to integer
- [x]	Create a new column
- [x]	Making a Boolean
- [x]	Strings and factors
- [x]	Carrier
- [x]	Destination
- [x]	Assemble a vector
- [x]	Create the pipeline
- [x]	Test vs Train
- [x]	Transform the data
- [x]	Split the data

### Model tuning and selection

- [x] What is logistic regression?
- [x] Create the modeler
- [x] Cross validation
- [x] Create the evaluator
- [x] Make a grid
- [x] Make the validator
- [x] Fit the model(s)
- [x] Evaluating binary classifiers
- [x] Evaluate the model

# Big Data Fundamentals with PySpark

[Certificate](https://www.datacamp.com/statement-of-accomplishment/course/06530532e301ebca7a6b5507637918b1d61592b9)

### Introduction to Big Data analysis with Spark

- [x] What is Big Data?
- [x] The 3 V's of Big Data
- [x] PySpark: Spark with Python
- [x] Understanding SparkContext
- [x] Interactive Use of PySpark
- [x] Loading data in PySpark shell
- [x] Review of functional programming in Python
- [x] Use of lambda() with map()
- [x] Use of lambda() with filter()

### Programming in PySpark RDDâ€™s

- [x] Abstracting Data with RDDs
- [x] RDDs from Parallelized collections
- [x] RDDs from External Datasets
- [x] Partitions in your data
- [x] Basic RDD Transformations and Actions
- [x] Map and Collect
- [x] Filter and Count
- [x] Pair RDDs in PySpark
- [x] ReduceBykey and Collect
- [x] SortByKey and Collect
- [x] Advanced RDD Actions
- [x] CountingBykeys
- [x] Create a base RDD and transform it
- [x] Remove stop words and reduce the dataset
- [x] Print word frequencies

### PySpark SQL & DataFrames

- [x] Abstracting Data with DataFrames
- [x] RDD to DataFrame
- [x] Loading CSV into DataFrame
- [x] Operating on DataFrames in PySpark
- [x] Inspecting data in PySpark DataFrame
- [x] PySpark DataFrame subsetting and cleaning
- [x] Filtering your DataFrame
- [x] Interacting with DataFrames using PySpark SQL
- [x] Running SQL Queries Programmatically
- [x] SQL queries for filtering Table
- [x] Data Visualization in PySpark using DataFrames
- [x] PySpark DataFrame visualization
- [x] Part 1: Create a DataFrame from CSV file
- [x] Part 2: SQL Queries on DataFrame
- [x] Part 3: Data visualization


### Machine Learning with PySpark MLlib

- [x] Overview of PySpark MLlib
- [x] PySpark ML libraries
- [x] PySpark MLlib algorithms
- [x] Collaborative filtering
- [x] Loading Movie Lens dataset into RDDs
- [x] Model training and predictions
- [x] Model evaluation using MSE
- [x] Classification
- [x] Loading spam and non-spam data
- [x] Feature hashing and LabelPoint
- [x] Logistic Regression model training
- [x] Clustering
- [x] Loading and parsing the 5000 points data
- [x] K-means training
- [x] Visualizing clusters


# Cleaning Data with PySpark

### DataFrame details

- [x] A review of DataFrame fundamentals and the importance of data cleaning.
- [x] Intro to data cleaning with Apache Spark
- [x] Data cleaning review
- [x] Defining a schema
- [ ] Immutability and lazy processing
- [ ] Immutability review
- [ ] Using lazy processing
- [ ] Understanding Parquet
- [ ] Saving a DataFrame in Parquet format
- [ ] SQL and Parquet

### Manipulating DataFrames in the real world

- [ ] DataFrame column operations
- [ ] Filtering column content with Python
- [ ] Filtering Question #1
- [ ] Filtering Question #2
- [ ] Modifying DataFrame columns
- [ ] Conditional DataFrame column operations
- [ ] when() example
- [ ] When / Otherwise
- [ ] User defined functions
- [ ] Understanding user defined functions
- [ ] Using user defined functions in Spark
- [ ] Partitioning and lazy processing
- [ ] Adding an ID Field
- [ ] IDs with different partitions
- [ ] More ID tricks

### Improving Performance

- [ ] Caching
- [ ] Caching a DataFrame
- [ ] Removing a DataFrame from cache
- [ ] Improve import performance
- [ ] File size optimization
- [ ] File import performance
- [ ] Cluster configurations
- [ ] Reading Spark configurations
- [ ] Writing Spark configurations
- [ ] Performance improvements
- [ ] Normal joins
- [ ] Using broadcasting on Spark joins
- [ ] Comparing broadcast vs normal joins

### Complex processing and data pipelines

- [ ] Introduction to data pipelines
- [ ] Quick pipeline
- [ ] Pipeline data issue
- [ ] Data handling techniques
- [ ] Removing commented lines
- [ ] Removing invalid rows
- [ ] Splitting into columns
- [ ] Further parsing
- [ ] Data validation
- [ ] Validate rows via join
- [ ] Examining invalid rows
- [ ] Final analysis and delivery
- [ ] Dog parsing
- [ ] Per image count
- [ ] Percentage dog pixels
