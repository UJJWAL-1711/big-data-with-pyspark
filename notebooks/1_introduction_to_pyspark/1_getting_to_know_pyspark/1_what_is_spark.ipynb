{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3769155a-35aa-4d5e-ac3a-30a4aa2a6b08",
   "metadata": {},
   "source": [
    "# What is Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33817cdc-da73-4374-88db-86fc654cb8de",
   "metadata": {},
   "source": [
    "Spark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30450946-ee1b-4ae9-9604-a5a8cf41765a",
   "metadata": {},
   "source": [
    "As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computations are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6659107-7e4a-4da7-9fe4-8cd29bbd7e6f",
   "metadata": {},
   "source": [
    "Deciding whether Spark is the best solution:\n",
    "- Is my data too big to work with on a single machine?\n",
    "- Can my calculations be easily parallelized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87a35b-26f8-43cf-b5cd-0192dec55da4",
   "metadata": {},
   "source": [
    "How do you connect to a Spark cluster from PySpark?\n",
    "- Create an instance of the SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2bb9b-06e7-4459-bba0-47e3332f7f1b",
   "metadata": {},
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e589854-dbef-46d0-80fd-db8564f32f21",
   "metadata": {},
   "source": [
    "Spark's core data structure is the RDD (Resilient Distributed Dataset). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you'll be using the Spark DataFrame abstraction built on top of RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32751855-395f-4dcc-a812-ec6034cc7165",
   "metadata": {},
   "source": [
    "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e0261f-a5d9-4c19-834b-6e1c981567fa",
   "metadata": {},
   "source": [
    "Which of the following is an advantage of Spark DataFrames over RDDs?\n",
    "- Operations using DataFrames are automatically optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3226a6-4301-4805-9ff7-88f16f5447e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dcee194-c347-4133-8a9e-00c13ac41aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('big_data_with_pyspark')\\\n",
    "    .config('spark.jars', '../../../jars/snowflake-jdbc-3.13.6.jar,../../../jars/spark-snowflake_2.12-2.9.0-spark_3.1.jar') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv('../../../datasets/flights.csv', header='true',)\n",
    "df.createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6902b52-60e6-4caa-9191-ee18fb61d98a",
   "metadata": {},
   "source": [
    "One of the advantages of the df interface is that you can run SQL queries on the tables in Spark cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b34e291d-189b-40f4-a749-6825396f3716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Don't change this query\n",
    "query = \"FROM flights SELECT * LIMIT 10\"\n",
    "\n",
    "# Get the first 10 rows of flights\n",
    "flights10 = spark.sql(query)\n",
    "\n",
    "# Show the results\n",
    "flights10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c60451-193a-4d09-a22f-f19e53a96d03",
   "metadata": {},
   "source": [
    "### Pandafy a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00fe4d57-fe7d-40b0-b836-eb5b343cffb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
    "flight_counts = spark.sql(query)\n",
    "pd_counts = flight_counts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35bb0135-5580-449c-92fd-8ebbf66e905b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEA</td>\n",
       "      <td>RNO</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SEA</td>\n",
       "      <td>DTW</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SEA</td>\n",
       "      <td>CLE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SEA</td>\n",
       "      <td>LAX</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PDX</td>\n",
       "      <td>SEA</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  origin dest    N\n",
       "0    SEA  RNO    8\n",
       "1    SEA  DTW   98\n",
       "2    SEA  CLE    2\n",
       "3    SEA  LAX  450\n",
       "4    PDX  SEA  144"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_counts.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
